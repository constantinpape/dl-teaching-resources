{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3.bkp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "3_multi_layer_perceptron.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B8zAmXMzQa5",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/constantinpape/training-deep-learning-models-for-vison/blob/master/day1/3_multi_layer_perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr3oCM0LzQa8",
        "colab_type": "text"
      },
      "source": [
        "# Multi-layer Perceptron on CIFAR10\n",
        "\n",
        "Based on the previous exercise, we will train a multi-layer network, also known as multi-layer perceptron. In this architecture, we will have 'hidden layers', i.e. layers that receive input not from the input directly but from previous layers in the network and that are not directly observed in the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra1bNlFN4qoz",
        "colab_type": "text"
      },
      "source": [
        "## Preperation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyXMvcuCzQbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load tensorboard extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX0lcNv2zQba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import torch and other libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import sklearn.metrics as metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import trange\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLbdr5lyzQbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install cifar2png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_adA5sMRzQb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check if we have gpu support\n",
        "# colab offers free gpus, however they are not activated by default.\n",
        "# to activate the gpu, go to 'Runtime->Change runtime type'. \n",
        "# Then select 'GPU' in 'Hardware accelerator' and click 'Save'\n",
        "have_gpu = torch.cuda.is_available()\n",
        "# we need to define the device for torch, yadda yadda\n",
        "if have_gpu:\n",
        "    print(\"GPU is available\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"GPU is not available, training will run on the CPU\")\n",
        "    device = torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcngiR2HzQb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run this in google colab to get the utils.py file\n",
        "!wget https://raw.githubusercontent.com/constantinpape/training-deep-learning-models-for-vison/master/day1/utils.py "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxJnmv9OzQcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we will reuse the training function, validation function and\n",
        "# data preparation from the previous notebook\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s790zEKzQcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cifar_dir = './cifar10'\n",
        "!cifar2png cifar10 cifar10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK8eKTTSzQcb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categories = os.listdir('./cifar10/train')\n",
        "categories.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaWkklrjzQci",
        "colab_type": "text"
      },
      "source": [
        "## Multi-layer perceptron\n",
        "\n",
        "In this execise, we go from a single layer network to a network with multiple layers, also known as multi-layer perceptron, or MLP.\n",
        "We still use fully connected layers (`nn.Linear`), i.e. each neuron in a given layer receives input from all neurons in the previous layer.\n",
        "\n",
        "Imporantly, we apply a non-linearity to all layer outputs.\n",
        "Otherwise, the layers could be collapsed into a single layer by matrix multiplication."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiMGapv9zQck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_pixels, n_classes):\n",
        "        super().__init__()\n",
        "        self.n_pixels = n_pixels\n",
        "        self.n_classes = n_classes\n",
        "        \n",
        "        # here, we define the structure of the MLP.\n",
        "        # it's imporant that we use a non-linearity after each \n",
        "        # fully connected layer! Here we use the rectified linear\n",
        "        # unit, short ReLu\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(n_pixels, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, n_classes),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.n_pixels)\n",
        "        x = self.layers(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXIWFz0AzQcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instantiate the model\n",
        "model = MLP(3072, 10)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iVu6jS9zQcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get training and validation data\n",
        "train_dataset, val_dataset = utils.make_cifar_datasets(cifar_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OejyDJzOzQc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instantiate loaders, loss, optimizer and tensorboard\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=25)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1.e-3)\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "loss_function.to(device)\n",
        "\n",
        "tb_logger = SummaryWriter('runs/log_mlp')\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka_zkBLdzQdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train for a couple of epochs\n",
        "n_epochs = 4\n",
        "for epoch in trange(n_epochs):\n",
        "    utils.train(model, train_loader, loss_function, optimizer,\n",
        "                device, epoch, tb_logger=tb_logger)\n",
        "    step = (epoch + 1) * len(train_loader)\n",
        "    utils.validate(model, val_loader, loss_function,\n",
        "                   device, step,\n",
        "                   tb_logger=tb_logger)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAINshVEzQdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate the model on test data\n",
        "test_dataset = utils.make_cifar_test_dataset(cifar_dir)\n",
        "test_loader = DataLoader(test_dataset, batch_size=25)\n",
        "predictions, labels = utils.validate(model, test_loader, loss_function,\n",
        "                                     device, step=0, tb_logger=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDsUV2PmzQdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(\"Test accuracy:\")\n",
        "accuracy = metrics.accuracy_score(labels, predictions)\n",
        "print(accuracy)\n",
        "\n",
        "fig, ax = plt.subplots(1, figsize=(8, 8))\n",
        "utils.make_confusion_matrix(labels, predictions, categories, ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQUvUBL2zQdd",
        "colab_type": "text"
      },
      "source": [
        "## Tasks and Questions\n",
        "\n",
        "Tasks\n",
        "- Try 3 different learning rate values and observe how this changes the training curves, training time and results on the test data.\n",
        "- Try different architectures for your MLP. For example, you can increase the number of layers or use a different activation function (e.g. Sigmoid) and compare them on the test dataset.\n",
        "\n",
        "Question:\n",
        "- Which of the models you have trained in this and in the previous notebook perform best?\n",
        "- Besides the final performance, how do the training curves differ?\n",
        "- Can you find any systematic similarities or differences in the confusion matrix for the different models?\n",
        "- What can you do to make the results of these comparisons more reliable?\n",
        "\n",
        "Advanced:\n",
        "- Try one or more of your architectures with preset filters (see last notebook) as input features."
      ]
    }
  ]
}